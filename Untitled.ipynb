{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 662,
   "id": "ad04a6af-ec88-4260-981b-61f94a1aa48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression,  Ridge\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.api import OLS\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "id": "7d8ccbc1-bbde-41a4-9cf6-42cec3c7cea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_path, test_path):\n",
    "    train_data = pd.read_csv(train_path)\n",
    "    test_data = pd.read_csv(test_path)\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "7e7957f2-09a4-43d3-b6f7-f4dc479ad208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data(data, features):\n",
    "    for feature in features:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "\n",
    "        # Boxplot for outlier detection\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.boxplot(data[feature])\n",
    "        plt.title(f'Boxplot of {feature}')\n",
    "\n",
    "        # Histogram for distribution\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.histplot(data[feature], kde=True)\n",
    "        plt.title(f'Distribution of {feature}')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "31d610db-dd6c-49d4-94d8-2ac0df2ba1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals(residuals, y_pred):\n",
    "    plt.scatter(y_pred, residuals)\n",
    "    plt.title('Residuals vs. Predicted Values')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "1be8e7ef-6633-4627-9696-c9e68d5eebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normality(residuals):\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "    plt.title('Normal Q-Q plot')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "id": "3e2e1238-29c1-4803-ba7d-126cf639702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, features, imputer):\n",
    "    data[features] = imputer.transform(data[features])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "id": "94c524d0-c521-46c2-8882-003565fe51ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_statsmodels(X, y):\n",
    "    X = sm.add_constant(X)  # Adding a constant to the model\n",
    "    model = sm.OLS(y, X).fit(cov_type='HC0')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "31c2da20-64bb-44e8-a084-820ef56dbfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_sklearn(X, y):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "bf50669d-8178-4e4f-8898-15abd06653da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_ridge(X, y):\n",
    "    model = Ridge()\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "id": "905a10bf-caf9-41ba-944b-aa2a9e145cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_lasso(X, y):\n",
    "    model = Lasso()\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "d706bc2d-4c17-477c-8ee8-45ddf1884315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_enet(X, y):\n",
    "    model = ElasticNet()\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "c1f6d07b-6402-48ae-b679-436bf6b1a39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "id": "7e8f89d0-94b0-4660-a48c-18f39993dc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Paths to the datasets\n",
    "    train_path = 'train.csv'\n",
    "    test_path = 'test.csv'\n",
    "\n",
    "    # Load the data\n",
    "    train_data, test_data = load_data(train_path, test_path)\n",
    "    \n",
    "    # Visualize data\n",
    "    selected_features = ['LotArea','YrSold', 'OverallQual', 'GrLivArea', 'TotalBsmtSF','GarageCars', 'GarageArea', 'MSSubClass', 'YearBuilt']\n",
    "    #visualize_data(train_data, selected_features)\n",
    "\n",
    "    # Handling missing values\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    train_data[selected_features] = imputer.fit_transform(train_data[selected_features])\n",
    "\n",
    "    # Outlier removal\n",
    "    train_data = train_data.drop(train_data[(train_data['GrLivArea']>4000) & (train_data['SalePrice']<300000)].index)\n",
    "\n",
    "    # Apply log transformation to the target variable 'SalePrice'\n",
    "    train_data['SalePrice'] = np.log1p(train_data['SalePrice'])\n",
    "    y = train_data['SalePrice']\n",
    "\n",
    "    # Splitting the train data into X (features) and y (target)\n",
    "    X = train_data[selected_features]\n",
    "\n",
    "    # Polynomial features\n",
    "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    \n",
    "    # Splitting the data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_poly, y, test_size=0.2, random_state=0)\n",
    "    \n",
    "    # Training the models\n",
    "    model_sl = train_model_sklearn(X_train, y_train)\n",
    "    model_r = train_model_ridge(X_train, y_train)\n",
    "    model_l = train_model_lasso(X_train, y_train)\n",
    "    model_en = train_model_enet(X_train, y_train)\n",
    "    model_sm = train_model_statsmodels(X_train, y_train)\n",
    "    models = AveragingModels(models = (model_sl, model_r, model_l, model_en, model_sm))\n",
    "\n",
    "    # Evaluating the model\n",
    "    X_val = sm.add_constant(X_val)  # Adding a constant to the validation data\n",
    "    y_pred_log = models.predict(X_val)  # Predicted log-transformed prices\n",
    "    y_pred = np.expm1(y_pred_log)  # Inverse transformation\n",
    "    r_squared = r2_score(np.expm1(y_val), y_pred)\n",
    "    residuals = np.expm1(y_val) - y_pred\n",
    "    plot_residuals(residuals, y_pred)\n",
    "    normality(residuals)\n",
    "    print(\"R-squared value:\", r_squared)\n",
    "\n",
    "    # Preprocessing the test data\n",
    "    test_data = preprocess_data(test_data, selected_features, imputer)\n",
    "\n",
    "    # Polynomial features\n",
    "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "    X_test = poly.fit_transform(test_data[selected_features])\n",
    "    \n",
    "    # Predicting the housing prices for the test data\n",
    "    X_test = sm.add_constant(X_test)  # Adding a constant to the test data\n",
    "    predicted_log_prices = models.predict(X_test)  # Predicted log-transformed prices for test data\n",
    "    predicted_prices = np.expm1(predicted_log_prices)  # Inverse transformation for test data predictions\n",
    "    ''\n",
    "    # Saving the predictions\n",
    "    predicted_prices_df = pd.DataFrame({\n",
    "        'Id': test_data['Id'],\n",
    "        'SalePrice': predicted_prices\n",
    "    })\n",
    "    predicted_prices_df.to_csv('predicted_housing_prices_statsmodels.csv', index=False)\n",
    "    ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "id": "25f8d0b9-738a-4e79-92cd-4a2ca1e2e72b",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'model_sl' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[675], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[674], line 35\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(X_poly, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Training the models\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m models \u001b[38;5;241m=\u001b[39m AveragingModels(models \u001b[38;5;241m=\u001b[39m (\u001b[43mmodel_sl\u001b[49m, model_r, model_l, model_en, model_sm))\n\u001b[0;32m     36\u001b[0m model_sl \u001b[38;5;241m=\u001b[39m train_model_sklearn(X_train, y_train)\n\u001b[0;32m     37\u001b[0m model_r \u001b[38;5;241m=\u001b[39m train_model_ridge(X_train, y_train)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'model_sl' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
